---
title: "HW5"
author: "Brian Chang"
date: "2019/11/8"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Homework

1. Attempt a regression tree-based method (not covered in this tutorial) on a reasonable dataset of your choice. Explain the results. 

2. Attempt both a bagging and boosting method on a reasonable dataset of your choice. Explain the results.

```{r load relevant libraries, include=FALSE}
library(tidyverse)
library(ISLR)
library(tree)
library(randomForest)
library(MASS)
library(gbm)
library(mlbench)
library(rpart)
library(rpart.plot)
library(rsample)
library(ipred)
library(caret)
```

## Regression Tree
* Our regression tree is trying to predict diabetes positive or negative based on 8 variables.
* The output from our regression tree shows the number of the node in descending order.
  + For example, the first split is at node '2)' with the variable glucose at <127.5 with 485 observations.
  + The corresponding deviance value at each node is the likelihood-ratio for testing the null hypothesis against the general alternative, or saturated model.
  + Terminal nodes are marked with a *
* We can also look at the actual tree for easier visualization.
* The cptable shows the relative error at differing numbers of terminal nodes.
  + We see in the cptable that from node 1 to node 2, or the first split, the relative error dropped from 1 to 0.818.
  + Each sequential node drops the relative error and the model stops after the error drop is <0.01, the default limit.
* The cp plot shows the lowest cross validation error as "x-val relative error.' 
  + The complexity parameter, or CP, is used to optimize the size of the tree. 
  + If the cost of adding another variable to the tree from the current node is above the value of the CP, the tree does not continue to split.
  + This coincides with the output of the tree as we can see that there were 6 levels of nodes and the xerror is lowest at CP 6.
```{r regression}
data(PimaIndiansDiabetes)

regression <- rpart(formula = diabetes ~ ., 
                    data = PimaIndiansDiabetes, 
                    method = "anova"
                    )
regression

cptable <- regression$cptable
print(cptable)

# plot of regression tree
rpart.plot(regression)
# plot of cp table and relative error
plotcp(regression)
```


## Bagging
* In this bagging approach, we are building tree models on bootstrapped datasets and then aggregating them, resulting in a model with reduced variance.
* After aggregating and building a single model, we then apply that model to our test set to predict our outcome of interest.
* The OOB estimate, or error, are calculated from the observations left out in the bootstrap samples to estimate the misclassifcation error.
  + Our test error is consistently higher than the OOB error but not by much so it is a good estimate of the true test error.
  + We can also see that the OOB error stabilizes at ~90-100 trees.
```{r bagging}
data(Carseats)
#split into train and test sets (300 and 100 respectively)
set.seed(200)
car_train = sample(1:nrow(Carseats), 300)
rf.car = randomForest(Sales ~ ., data = Carseats, subset = car_train)
rf.car

oob.err = double(13)
test.err = double(13)

#In a loop of mtry from 1 to 13, you first fit the randomForest to the train dataset
for(mtry in 1:13){
  fit = randomForest(Sales~., data = Carseats, subset=car_train, mtry=mtry, ntree = 350) #x`fit model to data
  oob.err[mtry] = fit$mse[350] ##extract Mean-squared-error 
  pred = predict(fit, Carseats[-car_train,]) #predict on test dataset
  test.err[mtry] = with(Carseats[-car_train,], mean((Sales-pred)^2)) #compute test error
}

#Visualize 
matplot(1:mtry, cbind(test.err, oob.err), pch = 23, col = c("red", "blue"), type = "b", ylab="Mean Squared Error")
legend("topright", legend = c("OOB", "Test"), pch = 23, col = c("red", "blue"))
# plot oob error vs. number of trees
plot(rf.car$mse, type='l', col=2, lwd=2, xlab='Number of Trees', ylab = 'OOB Error')
```



## Boosting
* Trees are trained sequentially with information from prior trees.
* Each tree is fitted on modified data with updated residuals.
  + Boosting can overfit if the number of trees are large.
  + The shrinkage perimeter determines the rate at which boosting learns.
* We get an output of the variables and their corresponding weight, or relative influence.
  + Partial dependence plots are shown for "price" and "age," which show the predicted outcome at various values of "price" and "age."
* In the final plot, we see that the MSE is at its lowest at ~900 trees. We should build our model at that many number of trees.
  + The red line shows the lowest MSE value.
```{r boosting}
set.seed(123)

smp_size <- floor(0.75 * nrow(Carseats))
train_car <- sample(seq_len(nrow(Carseats)), size = smp_size)
train <- Carseats[train_car, ]
test <- Carseats[-train_car, ]

#Gradient Boosting Model
boost.car = gbm(Sales~., data = Carseats[train_car,], distribution = "gaussian", n.trees = 10000, shrinkage = 0.01, interaction.depth = 4)

#Variable Importance Plot
summary(boost.car)

#partial dependence plots on most important variables
plot(boost.car,i="Price")
plot(boost.car,i="Age")

#Predict on test set
n.trees = seq(from = 100, to = 10000, by = 100)
predmat = predict(boost.car, newdata = Carseats[-train_car,], n.trees = n.trees)
dim(predmat)

#Visualize Boosting Error Plot
boost.err = with(Carseats[-train_car,], apply((predmat - Sales)^2, 2, mean))
plot(n.trees, boost.err, pch = 23, ylab = "Mean Squared Error", xlab = "# Trees", main = "Boosting Test Error")
abline(h = min(test.err), col = "red")
```

